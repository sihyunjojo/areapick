# 🏗 프로젝트 폴더 구조 및 MSA 아키텍처

## 📂 프로젝트 폴더 구조

```plaintext
📂 프로젝트 루트
├── 📂 alert              # 알림 API 서버
├── 📂 hadoop-cluster     # 하둡 클러스터 설정 파일 및 문서
├── 📂 log-collector      # 로그 수집 및 저장 API 서버
├── 📂 msa-infrastructure # Eureka, Config, Gateway 서버 등 MSA 관리
├── 📂 recommendation     # 상권 및 게시판 검색어 추천 API 서버
└── 📂 total-repo         # API 서버 (알림, 추천, 로그 수집 제외) + Vue.js 프론트엔드
```


## 🏛 MSA 구성 및 서비스 분리 이유
본 프로젝트는 **마이크로서비스 아키텍처(MSA)** 기반으로 설계되었으며, 각 서비스는 독립적인 역할을 수행합니다.

### 🔔 **Alert (알림 API 서버)**
- **푸시 알림을 효율적으로 관리하기 위해 독립적인 서비스로 운영**
- **비즈니스 로직과 분리하여 알림 트래픽 증가 시 API 서버에 영향을 주지 않도록 설계**
- **Kafka와 같은 메시지 큐를 활용해 비동기 방식으로 알림 전송 최적화**

### 📊 **Log Collector (로그 수집 및 저장 API 서버)**
- **서비스 간 요청 및 사용자의 활동 로그를 효율적으로 관리하기 위해 별도 서버 운영**
- **데이터 분석 및 모니터링을 위해 로그 데이터를 저장하고 분석할 수 있도록 설계**
- **알림 서버와도 연결되어 있어, 특정 이벤트 발생 시 알림 트리거 역할 수행 가능**

### 🔍 **Recommendation (검색어 추천 API 서버)**
- **검색 중심 사용자 경험이 예상되는 프로젝트이므로 검색 기능을 독립적인 서비스로 분리**
- **검색어 추천, 상권 분석, 게시판 추천 기능을 별도 서버에서 관리하여 확장성 확보**
- **검색어 데이터와 추천 알고리즘을 최적화하여 사용자 맞춤형 검색 환경 제공**

### 🌎 **Total Repo (주요 API 서버 및 Vue.js 프론트엔드)**
- **로그 수집, 검색 추천, 알림을 제외한 핵심 기능을 담당**
- **MSA 구조에서 Gateway와 연결되어 백엔드 API의 진입점 역할 수행**
- **Vue.js로 작성된 프론트엔드를 함께 포함하여 일관된 구조 유지**
### 

# 상추:창고 <br>상권 추천 창업 레츠고
SSAFY 11기 특화 프로젝트 

## 📜 Contents
 1. [프로젝트 개요](#1️⃣프로젝트-개요)
 2. [주요 기능](#2️⃣주요-기능)
 3. [개발 환경](#3️⃣개발-환경)
 4. [기술 특이점](#4️⃣기술-특이점)
 5. [산출물](#5️⃣산출물)
 6. [팀원 소개](#6️⃣팀원-소개)

## 1️⃣프로젝트 개요
### 프로젝트 기간
2024.08.19 ~ 2024.10.11 (8주)

### 프로젝트 소개
상추:창고는 프랜차이즈 창업을 희망하는 사람들에게 원하는 지역 상권 및 프랜차이즈에 대한 다양한 정보와 분석을 제공하여 성공적인 창업의 길을 더욱 쉽게 열어갈 수 있습니다.

### 기획의도
프랜차이즈 시장의 급성장으로 인해 경쟁이 심화되고 **진입장벽이 높아지고 있습니다**. 많은 사람들이 유행을 따라 창업에 나서지만, 충분한 조사 없이 성급하게 시작할 경우 실패할 위험이 큽니다. 유행의 속도가 가속화되면서 신규 프랜차이즈 업체의 수도 급증한 지금, 내가 선택한 프랜차이즈가 장기간 안정적인 수익을 창출할 수 있을지를 철저히 검토하는 것이 필요합니다.

 `상추창고`는 **신중한 시장 진입과 철저한 상권 분석**에 중점을 둡니다. 또한, 정확한 분석과 함께 같은 고민을 공유하는 사람들과의 커뮤니티를 형성하여 생생한 정보를 업데이트하고 교류하며 창업자들이 보다 안정적인 환경에서 사업을 시작할 수 있도록 돕는 것을 목표로 합니다.

## 2️⃣주요 기능 
- 빅데이터를 활용한 상권 분석
- 프랜차이즈 창업 비용 예측
- 상권 및 프랜차이즈 커뮤니티
- 상권 및 게시판 검색어 추천 


## 3️⃣개발 환경

|   TYPE    | SKILL           | VERSION          |
| :-------: | :-------------- | :--------------- |
| LANGUAGES | Java            | 17           |
|           | Python          | 3.12.4              |
|           | Javascript      | ES6              |
| BE | Sprint boot     | 3.3.2           |
|           | Spring Data JPA |                  |
|           | querydsl |       5.0.0           |
|  FE         | Vue           | 3.4.29               |
| DATABASE  | MariaDB           | 8.0              |
|           | Redis           |                  |
|   INFRA   | AWS EC2         | ubuntu(x86, arm) |
|           | Jenkins         |        2.479          |
|           | Docker          |        24.0.7         |
|           | redis-cli           |      7.4.0            |
|           | hadoop           |      3.3.1           |
|           | spark           |         3.5.2         |




## 4️⃣기술 특이점 


- ### 하둡을 사용한 분산 처리
    - Hadoop 클러스터 서버에서 Spark를 사용하여 데이터 전처리 및 데이터 저장
    - 분산 처리(sacle out)를 이용하여서 빠른 처리와 fault-tolerant(고장이나 오류가 강한 능력)한 장점을 얻었습니다.
    - `HDFS`는 하둡 분산 저장 파일 시스템으로, Hadoop 에서는 파일(데이터) 를 Block 단위로 관리. 하나의 파일을 여러개의 Block으로 쪼개고 이 Block들을 여러 nodes 들에 분산하여 저장하는 원리로, 이 프로젝트에서는 아래 정보들에 대해 분산 저장시킴.
  
    - 상권 분석 정보
        - 유동인구: 상권 별 연령별, 분기별 유동인구
        - 매출: 상권과 서비스 업종 별 연령별, 분기별 매출
        
    - `Spark`의 사용 : Hadoop 의 Map Reduce 기술은 빅데이터 분석을 쉽게 만들어주었지만, 복잡한 처리(머신러닝이나 그래프 분석) 진행시 HDFS에 데이터를 읽고 쓰는 연산이 많아 성능 저하가 일어난다는 단점이 있다. 
    
        - Disk 대신 빠른 RAM을 사용하여 데이터를 처리해보자는 아이디어가 `Spark`이고, 이러한 RAM의 특성에 최적화하여 효율적인 방식으로 처리
        - `RDD` 방식을 사용하여서 메모리를 활용해 데이터를 처리하면서도, 장애 복구를 지원하여 중간에 문제가 생겨도 다른 노드에 있는 데이터를 이용해 복구가 가능
    
        - 버전이 올라감에 따라 RDD와 같은 저수준 API보다 DataFrame, DataSet과 같은 고수준 API를 사용하며 이 프로젝트에선 DataFrame API를 사용
        - `DataFrame`은 행과 열로 구성된 데이터 구조로, 테이블처럼 데이터를 다룰 수 있습니다. SQL 쿼리처럼 데이터를 필터링하거나, 집계할 수 있어서 데이터 분석에 훨씬 유리
        - DataFrame API를 사용하여 Spark는 내부적으로 RDD를 활용해 데이터를 처리하고 있습니다. 즉, 사용자는 고수준 API로 더 쉽게 코드를 작성할 수 있지만, Spark는 내부적으로 RDD를 사용해 메모리를 활용하고 분산처리

    #### 적용 코드
    - Spark Session으로 시작해 분산 저장 환경에서 데이터를 읽고, 처리하는 코드입니다. 
    - Spark의 해당 프로젝트에서는 `groupBy`와 `pivot` 연산을 사용하였습니다.
        - groupBy: 데이터를 그룹으로 묶고 그룹에 대해서 집계 연산을 사용하는데 사용하였습니다.
        - pivot: 특정 열의 값을 새로운 열로 만들고, 그 열에 대한 집계 값을 계산하는 데 사용하였습니다.
    - 기본적으로 많은 데이터를 처리할 때 각 워커 노드에서 병렬로 연산이 이루어지며, 그 결과를 shuffling(데이터를 다시 재배치하여 합산하는 과정)하여 집계합니다. 
    
        ```python
        # Spark 세션 시작
        spark = SparkSession.builder \
            .appName("CSV to MariaDB") \
            .getOrCreate()
            
        # CSV 파일 읽기
        area_df = spark.read.option("encoding", "cp949").csv("hdfs://master:9000/input/서울시_상권분석서비스(추정매출-상권).csv", header=True, inferSchema=True)
        
        # 필요한 컬럼 선택
        area_2023_2024 = area_df.select('기준_년분기_코드', '상권_코드', '상권_코드_명', '서비스_업종_코드', '서비스_업종_코드_명', '당월_매출_금액')
        
        # groupBy와 pivot을 사용해 기준_년분기_코드별 매출 합계 계산
        pivot_area = area_2023_2024.groupBy('상권_코드', '상권_코드_명', '서비스_업종_코드', '서비스_업종_코드_명') \
                                     .pivot('기준_년분기_코드') \
                                     .agg(F.sum('당월_매출_금액'))
        ```
    
        

- ### Elastic Search를 활용한 검색어 추천
    - 상권 검색어 추천 인덱스 
        - 색인 시, 접두사 부터 접근하는 방식과 형태소로 접근하는 방식에 모두 대응하기 위해서 `nori 분석기`와 `edge ngram` 분석기 사용
        - 검색 시, 상권의 경우 어느 정도 정보를 알고 단어의 앞부터 완벽한 단어로 검색할 것으로 예상하여 `edge ngram 분석기` 사용
    - 게시판 검색어 추천 인덱스
        - 프랜차이즈의 이름을 통해 검색하는 경우:
            - 색인 시, 프랜차이즈의 경우 이름이 정확하게 기억이 안날 확률이 높으므로 `표준 토크나이저`, `n-gram`, `edge ngram`, `nori 분석기`로 다양하게 사용 
            - 검색 시, 프랜차이즈 이름은 사전 정보의 한계 때문에 형태소 기반 분석이 어려움 그러하여 `ngram` 분석기와 `edge ngram` 분석기 사용
        - 상권의 이름을 통해 검색하는 경우:
            - 색인 시, 접두사 부터 접근하는 방식과 형태소로 접근하는 방식에 둘다 대응하기 위해서 `nori 분석기`와 `edge ngram 분석기` 사용
            - 검색 시, 상권의 경우 어느 정도 정보를 알고 단어의 앞부터 완벽한 단어로 검색할 것으로 예상하여 `edge ngram 분석기` 사용
        - 여러 분석기를 통해 나온 값을 정확도를 높히기 위해 가장 관련성 높은 결과를 우선순위로 두는 `Dis Max 쿼리` 사용 
    - 이후 검색의 다양성을 위해 두 인덱스 결과 중 **하나의 인덱스의 결과에서 최소 20퍼센트를 보장.**



## 5️⃣산출물 

# 프로젝트 전체 아키텍처

![msa 아키텍처](https://github.com/user-attachments/assets/2750a33d-b5f4-4a44-a1d5-15a80cf77bfc)

### 피그마
[figma](https://www.figma.com/design/tde91Y45Rt5Eoyb0AAtLZy/D108-%ED%8A%B9%ED%99%94%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8?node-id=0-1&node-type=canvas&t=2XqcXYVPhUk2vsCN-0)

### ERD
[ERD 상세 보기](https://www.erdcloud.com/d/3kkFXSxpjhoWZ3LYJ) 

## 6️⃣팀원 소개

|  이름  | 역할                                                                             |
| :----: | :------------------------------------------------------------------------------- |
| 조시현 |  팀장, Backend, Frontend          |
| 박동현 |  Backend, Frontend                |
| 조민기 |  Backend, Frontend, Data Infra    |
| 이승봉 |  Backend, Frontend, Infra         |
| 곽대건 |  Backend, Frontend                |
| 정진규 |  Backend, Frontend, Data Engineer |
| 김민솔 |  Backend, Frontend, Data Engineer |

